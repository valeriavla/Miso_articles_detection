[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Authors",
    "section": "",
    "text": "Josh Weiye Valeria"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Revictimization: a misogyny detection problem",
    "section": "",
    "text": "The responsibility of the communication platforms should be to objectively inform the public without attempting to influence their opinions, but so far, the press has been incapable of doing this. For example, Mexican media has stated, in news articles of occurred femicides, comments such as “It was her fault,” “Her parents should have picked her up,” “Surely she set herself on fire,” “Look how she was dressed up that day,” “She might have fallen and bumped her head” [1].\nThe media portrays women victims of violence as stigmatized, guilty, untrustworthy, or sexualized. This method of data governance creates a sense of revictimization in the survivors’ or witnesses’ families and fosters another form of violence against women: media violence.\nAn analysis done by [2] shows that out of 1,300 reports of violence against women published by more than 20 media platforms from Argentina, 897 revictimized the sufferer women. Such approaches present dangerous interpretive frameworks that justify violent acts, perpetrate a misogynistic and discriminative culture, or even jeopardize judicial investigations."
  },
  {
    "objectID": "index.html#data-labeling",
    "href": "index.html#data-labeling",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Data labeling:",
    "text": "Data labeling:\nThe text from each article will be split into sentences for them to be labeled as “misogynistic” or “non-misogynistic.” The article will be classified as misogynistic if a paragraph contains one misogynistic sentence."
  },
  {
    "objectID": "index.html#data-cleaning",
    "href": "index.html#data-cleaning",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Data cleaning:",
    "text": "Data cleaning:\nThe text will be cleaned through the standard NLP cleaning task, such as punctuation, numbers, hypertext and stop-words removals, and lowercase transformation for all characters.\nBased on the state-of-the-art results, lemmatization and stemming techniques have not improved the performance of sentiment classification in texts in Spanish; thus, multiple sets will be generated to test the effects of those techniques."
  },
  {
    "objectID": "index.html#data-modeling",
    "href": "index.html#data-modeling",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Data modeling:",
    "text": "Data modeling:\nDifferent word modeling techniques that have shown exemplary performance in Sentiment Analysis will be implemented. The work aims to do an exploratory analysis of word modeling techniques results for misogyny detection in news articles that revictimize women.\nTo do so, representations that lose semantic representation, such as BoW, will be implemented with techniques to solve problems, i.e., TF-IDF. Models that preserve semantic representation will also be tested, such as BERT and Word2Vec."
  },
  {
    "objectID": "index.html#data-classification",
    "href": "index.html#data-classification",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Data classification:",
    "text": "Data classification:\nMultiple data sets with tweets in Spanish, posted on México, and labeled had been gathered, creating a balanced data set of 10,000 tweets. A small dataset of articles will be manually-labeled to fine-tune a BERT model with the tweets dataset and evaluate its performance by classifying news articles."
  },
  {
    "objectID": "index.html#evaluation-criteria",
    "href": "index.html#evaluation-criteria",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Evaluation criteria:",
    "text": "Evaluation criteria:\nSince we have labeled data, results will be evaluated in terms of accuracy, precision, and a deep understanding of how the classifier is detecting both subtle and apparent misogyny by testing it with text examples labeled as “subtle” and “clear” misogyny."
  },
  {
    "objectID": "about.html#weiye",
    "href": "about.html#weiye",
    "title": "Authors",
    "section": "Weiye",
    "text": "Weiye"
  },
  {
    "objectID": "about.html#valeria-vera-lagos-vv188georgetown.edu",
    "href": "about.html#valeria-vera-lagos-vv188georgetown.edu",
    "title": "Authors",
    "section": "Valeria Vera Lagos vv188@georgetown.edu",
    "text": "Valeria Vera Lagos vv188@georgetown.edu"
  },
  {
    "objectID": "about.html#weiye-zhu",
    "href": "about.html#weiye-zhu",
    "title": "Authors",
    "section": "Weiye Zhu",
    "text": "Weiye Zhu"
  },
  {
    "objectID": "index.html#data-collection",
    "href": "index.html#data-collection",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Data collection:",
    "text": "Data collection:\nMaria Salguero is a Mexican activist who has collected multiple femicides articles published in Mexico. Her news and other articles will be manually collected to have a bast dataset."
  },
  {
    "objectID": "index.html#bibliography",
    "href": "index.html#bibliography",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Bibliography",
    "text": "Bibliography\n[1] “Fue su culpa: por qué es vital evitar la revictimización en los casos de feminicidio” https://www.infobae.com/america/mexico/2022/09/05/fue-su-culpa-por-que-es-vital-evitar-la-revictimizacion-en-los-casos-de-feminicidio/\n[2] “Medios revictimizantes”. Palacios, Claudia, https://www.eltiempo.com/opinion/columnistas/claudia-palacios/medios-revictimizantes-observatorio-de-medios-y-genero-149406\n[3] Chen, a. W. Global journal of advanced engineering technologies and sciences sentiment analysis of news articles and its comments: A Natural Language Processing Application, http://gjaets.com/Issues%20PDF/Archive-2018/May-2018/3.pdf"
  },
  {
    "objectID": "repository.html",
    "href": "repository.html",
    "title": "Repository",
    "section": "",
    "text": "Besides data whose ownership is not ours, the project’s scripts and data sets are open at our Github Repository"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Training a supervised classifier trained with a labeled set of tweets to predict if a tweet is misogynistic or not. Test the classifier with a set of tweets not used for training and test it with a small set of new articles to evaluate its precision. The objective is to find out if training models with tweets can be useful for different text entries.\nIn terms of models, as a baseline a NB model will be implemented to compare the results using BERT."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Obtention and Processing",
    "section": "",
    "text": "Data collection\nData labeling\nData cleaning\nData modeling"
  },
  {
    "objectID": "index.html#classification",
    "href": "index.html#classification",
    "title": "Revictimization: a misogyny detection problem",
    "section": "Classification:",
    "text": "Classification:\nMultiple data sets with tweets in Spanish, posted on México, and labeled had been gathered, creating a balanced data set of 10,000 tweets. A small dataset of articles will be manually-labeled to fine-tune a BERT model with the tweets dataset and evaluate its performance by classifying news articles."
  },
  {
    "objectID": "data_collection.html",
    "href": "data_collection.html",
    "title": "Data collection",
    "section": "",
    "text": "This work’s main objective is to classify news articles content and label it as misogynystic or not. However, no dataset contains such information; thus, a manual process must be performed to gather all information.\nManual obtention and labeling lead to a small amount of data, which is not helpful for models that require big datasets to be trained on, such as neural networks.\nSince there are multiple datasets containing tweets in Spanish labeled as misogynistic or not, we will solve the problem by utilizing such data to train models and analyze if those are useful for detecting revictimization."
  },
  {
    "objectID": "data_collection.html#tweets-collection",
    "href": "data_collection.html#tweets-collection",
    "title": "Data collection",
    "section": "Tweets collection",
    "text": "Tweets collection\nDifferent datasets collected and labeled through multiple works were gathered to have a 10,000 tweets dataset."
  },
  {
    "objectID": "data_collection.html#news-articles-collection",
    "href": "data_collection.html#news-articles-collection",
    "title": "Data collection",
    "section": "News articles collection",
    "text": "News articles collection\nMaria Salguero is a Mexican activist who has collected multiple femicides articles published in Mexico. To have a bast dataset, her set of almost 8 thousand news and a new collection performed through this work with @X news, will be labeled."
  },
  {
    "objectID": "data_labeling.html",
    "href": "data_labeling.html",
    "title": "Data labeling",
    "section": "",
    "text": "The process of labeling and training won’t be linear. A small set of news will be manually labeled, and the classifier will be tested with them to evaluate its performance. This will help to automatically classify the rest of the articles and retrain and evaluate the model."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data cleaning",
    "section": "",
    "text": "We have our original dataset labeled_set.csv with 2 columns, “text” and “label”, “text” column is the tweets we have gathered, and value 0 and 1 in “label” column represent the non misogynistic tweet and misogynistic tweet respetively.\nRight now the dataset is unuseble because there are way to many “noises” in our dataset, i.e. there are a lot of special characters, emojis, and website links in our text data, so we need to clean (remove) these “noises” before doing further analysis.\nHere are the data cleaning process step by step:\n\nImport necessary libraries and packages\n\n\nimport pandas as pd\nimport numpy as np\n\nimport nltk; \nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline  \nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nLoad the original dataset\n\n\ntweets= pd.read_csv('../code/labeled_set.csv', encoding = \"ISO-8859-1\")\ntweets.head(10)\n\n\n\n\n\n  \n    \n      \n      text\n      label\n    \n  \n  \n    \n      0\n      ?? no sé si das más pena tú o tu terrible ort...\n      0\n    \n    \n      1\n      Cuando todo se va al infierno, la gente que e...\n      0\n    \n    \n      2\n      - En 1800 nace #NatTurner, el esclavo rebelde ...\n      0\n    \n    \n      3\n      entre muchas otras  Muere el 6 de nov 2015 ??...\n      0\n    \n    \n      4\n      era la maldición de muchas familias.\n      0\n    \n    \n      5\n      https://t.co/I8JcEowZed #FelizLunes #FelizLun...\n      0\n    \n    \n      6\n      La rola de #LaLocaDelSenado\n      0\n    \n    \n      7\n      oh Dios mío!, desde cuando tienen eso ?\n      0\n    \n    \n      8\n      sexistas y totalmente inadecuados para su eda...\n      0\n    \n    \n      9\n      - Te ves bien crudo, ¿estuvo bueno el fin? - N...\n      0\n    \n  \n\n\n\n\n\nRemove empty space in columns if there is any. Rename the column name and check the data type of each column\n\n\ntweets.columns = tweets.columns.str.replace(' ', '')\ntweets.rename(columns={'text': 'tweets'}, inplace=True)\ntweets.dtypes\ntweets.head(5)\n\n\n\n\n\n  \n    \n      \n      tweets\n      label\n    \n  \n  \n    \n      0\n      ?? no sé si das más pena tú o tu terrible ort...\n      0\n    \n    \n      1\n      Cuando todo se va al infierno, la gente que e...\n      0\n    \n    \n      2\n      - En 1800 nace #NatTurner, el esclavo rebelde ...\n      0\n    \n    \n      3\n      entre muchas otras  Muere el 6 de nov 2015 ??...\n      0\n    \n    \n      4\n      era la maldición de muchas familias.\n      0\n    \n  \n\n\n\n\n\nRemove the “noises”\n\n\ntweets['tweets'] = tweets['tweets'].str.replace('[^\\w\\s]', '') # Remove all special symbols & characters\ntweets['tweets'] = tweets['tweets'].str.replace('_', '') # Remove all underscores\ntweets['tweets'] = tweets['tweets'].str.replace('http[^\\s]*',\"\") # Remove all words that start with \"http\"\ntweets['tweets'] = tweets['tweets'].astype(str).str.lower() # Make all words in lower case\ntweets.head(10)\n\nC:\\Users\\valer\\AppData\\Local\\Temp\\ipykernel_8264\\154057916.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n  tweets['tweets'] = tweets['tweets'].str.replace('[^\\w\\s]', '') # Remove all special symbols & characters\nC:\\Users\\valer\\AppData\\Local\\Temp\\ipykernel_8264\\154057916.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n  tweets['tweets'] = tweets['tweets'].str.replace('http[^\\s]*',\"\") # Remove all words that start with \"http\"\n\n\n\n\n\n\n  \n    \n      \n      tweets\n      label\n    \n  \n  \n    \n      0\n      no sé si das más pena tú o tu terrible ortog...\n      0\n    \n    \n      1\n      cuando todo se va al infierno la gente que es...\n      0\n    \n    \n      2\n      en 1800 nace natturner el esclavo rebelde del...\n      0\n    \n    \n      3\n      entre muchas otras  muere el 6 de nov 2015\n      0\n    \n    \n      4\n      era la maldición de muchas familias\n      0\n    \n    \n      5\n      felizlunes felizlunesatodos linux linuxsecur...\n      0\n    \n    \n      6\n      la rola de lalocadelsenado\n      0\n    \n    \n      7\n      oh dios mío desde cuando tienen eso\n      0\n    \n    \n      8\n      sexistas y totalmente inadecuados para su eda...\n      0\n    \n    \n      9\n      te ves bien crudo estuvo bueno el fin  no es ...\n      0\n    \n  \n\n\n\n\n\nSave the clean dataset\n\n\n# tweets.to_csv('cleaned_tweets.csv')\n# tweets.to_html('cleaned_tweets.html', classes='table table-stripped')\n\n\nNow we have the clean dataset so we can do some further analysis and some EDA.\nTokenization\n\n\nfrom nltk.tokenize import RegexpTokenizer\n\nregexp = RegexpTokenizer('\\w+')\n\ntweets['tweets_token']=tweets['tweets'].apply(regexp.tokenize)\ntweets.head(10)\n\n\n\n\n\n  \n    \n      \n      tweets\n      label\n      tweets_token\n    \n  \n  \n    \n      0\n      no sé si das más pena tú o tu terrible ortog...\n      0\n      [no, sé, si, das, más, pena, tú, o, tu, terrib...\n    \n    \n      1\n      cuando todo se va al infierno la gente que es...\n      0\n      [cuando, todo, se, va, al, infierno, la, gente...\n    \n    \n      2\n      en 1800 nace natturner el esclavo rebelde del...\n      0\n      [en, 1800, nace, natturner, el, esclavo, rebel...\n    \n    \n      3\n      entre muchas otras  muere el 6 de nov 2015\n      0\n      [entre, muchas, otras, muere, el, 6, de, nov, ...\n    \n    \n      4\n      era la maldición de muchas familias\n      0\n      [era, la, maldición, de, muchas, familias]\n    \n    \n      5\n      felizlunes felizlunesatodos linux linuxsecur...\n      0\n      [felizlunes, felizlunesatodos, linux, linuxsec...\n    \n    \n      6\n      la rola de lalocadelsenado\n      0\n      [la, rola, de, lalocadelsenado]\n    \n    \n      7\n      oh dios mío desde cuando tienen eso\n      0\n      [oh, dios, mío, desde, cuando, tienen, eso]\n    \n    \n      8\n      sexistas y totalmente inadecuados para su eda...\n      0\n      [sexistas, y, totalmente, inadecuados, para, s...\n    \n    \n      9\n      te ves bien crudo estuvo bueno el fin  no es ...\n      0\n      [te, ves, bien, crudo, estuvo, bueno, el, fin,...\n    \n  \n\n\n\n\n\nRemove infrequent words. We first change the format of tweets_token to strings and keep only words which are no shorter than 2 letters\n\n\ntweets['tweets_string'] = tweets['tweets_token'].apply(lambda x: ' '.join([item for item in x if len(item)>=2]))\ntweets.head(10)\n\n\n\n\n\n  \n    \n      \n      tweets\n      label\n      tweets_token\n      tweets_string\n    \n  \n  \n    \n      0\n      no sé si das más pena tú o tu terrible ortog...\n      0\n      [no, sé, si, das, más, pena, tú, o, tu, terrib...\n      no sé si das más pena tú tu terrible ortografí...\n    \n    \n      1\n      cuando todo se va al infierno la gente que es...\n      0\n      [cuando, todo, se, va, al, infierno, la, gente...\n      cuando todo se va al infierno la gente que est...\n    \n    \n      2\n      en 1800 nace natturner el esclavo rebelde del...\n      0\n      [en, 1800, nace, natturner, el, esclavo, rebel...\n      en 1800 nace natturner el esclavo rebelde del ...\n    \n    \n      3\n      entre muchas otras  muere el 6 de nov 2015\n      0\n      [entre, muchas, otras, muere, el, 6, de, nov, ...\n      entre muchas otras muere el de nov 2015\n    \n    \n      4\n      era la maldición de muchas familias\n      0\n      [era, la, maldición, de, muchas, familias]\n      era la maldición de muchas familias\n    \n    \n      5\n      felizlunes felizlunesatodos linux linuxsecur...\n      0\n      [felizlunes, felizlunesatodos, linux, linuxsec...\n      felizlunes felizlunesatodos linux linuxsecurit...\n    \n    \n      6\n      la rola de lalocadelsenado\n      0\n      [la, rola, de, lalocadelsenado]\n      la rola de lalocadelsenado\n    \n    \n      7\n      oh dios mío desde cuando tienen eso\n      0\n      [oh, dios, mío, desde, cuando, tienen, eso]\n      oh dios mío desde cuando tienen eso\n    \n    \n      8\n      sexistas y totalmente inadecuados para su eda...\n      0\n      [sexistas, y, totalmente, inadecuados, para, s...\n      sexistas totalmente inadecuados para su edad e...\n    \n    \n      9\n      te ves bien crudo estuvo bueno el fin  no es ...\n      0\n      [te, ves, bien, crudo, estuvo, bueno, el, fin,...\n      te ves bien crudo estuvo bueno el fin no es qu...\n    \n  \n\n\n\n\n\nCreate a list of all words\n\n\nall_words = ' '.join([word for word in tweets['tweets_string']])\n\n\nTokenize all_words\n\n\ntokenized_words = nltk.tokenize.word_tokenize(all_words)\n\n\nCreate a frequency distribution which records the number of times each word has occurred:\n\n\nfrom nltk.probability import FreqDist\nfdist = FreqDist(tokenized_words)\nfdist\n\nFreqDist({'que': 5199, 'la': 4773, 'de': 4715, 'no': 3236, 'el': 2861, 'es': 2462, 'en': 1967, 'una': 1720, 'las': 1680, 'se': 1599, ...})\n\n\n\nNow we can use our fdist dictionary to drop words which occur less than a certain amount of times (usually we use a value of 3 or 4).\n\n\ntweets['tweets_string_fdist'] = tweets['tweets_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] >= 4 ]))\n\n\nLemmatization\n\n\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lem = WordNetLemmatizer()\ntweets['tweets_string_lem'] = tweets['tweets_string_fdist'].apply(wordnet_lem.lemmatize)\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\nCheck if the columns are equal so we might not need to do the lemmatization\n\n\ntweets['is_equal']= (tweets['tweets_string_fdist']==tweets['tweets_string_lem'])\ntweets.is_equal.value_counts()\n\nTrue     10025\nFalse        4\nName: is_equal, dtype: int64\n\n\n\nLet’s create a word cloud to see what are the most frequent words\n\n\nall_words_lem = ' '.join([word for word in tweets['tweets_string_lem']])\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=600, \n                     height=400, \n                     random_state=2, \n                     max_font_size=100).generate(all_words_lem)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off');\n\n\n\n\n\nFrequency distributions\n\n\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\nwords = nltk.word_tokenize(all_words_lem)\nfd = FreqDist(words)\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\valer\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\nWe can list the top 15 most frequent words\n\n\nfd.most_common(15)\nfd.tabulate(15)\n\n que   la   de   no   el   es   en  una  las   se  los   me   un   lo  por \n5199 4776 4715 3236 2861 2462 1967 1720 1677 1599 1488 1357 1322 1258 1167 \n\n\n\nNow we can make a plot of the most frequent words\n\n\ntop_30 = fd.most_common(30)\n\n# Create pandas series to make plotting easier\nfdist = pd.Series(dict(top_30))\nimport seaborn as sns\nsns.set_theme(style=\"ticks\")\n\ntweets_top30_word_barplot = sns.barplot(y=fdist.index, x=fdist.values, color='pink')"
  },
  {
    "objectID": "about.html#weiye-zhu-wz250georgetown.edu",
    "href": "about.html#weiye-zhu-wz250georgetown.edu",
    "title": "Authors",
    "section": "Weiye Zhu wz250@georgetown.edu",
    "text": "Weiye Zhu wz250@georgetown.edu"
  }
]