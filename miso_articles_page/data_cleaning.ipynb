{
  "cells": [
    {
      "cell_type": "raw",
      "id": "8da694bd",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data cleaning\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af351e30",
      "metadata": {},
      "source": [
        "## Tweets cleaning\n",
        "We have our original dataset labeled_set.csv with 2 columns, \"text\" and \"label\", \"text\" column is the tweets we have gathered, and value 0 and 1 in \"label\" column represent the non misogynistic tweet and misogynistic tweet respetively. \n",
        "\n",
        "Right now the dataset is unuseble because there are way to many \"noises\" in our dataset, i.e. there are a lot of special characters, emojis, and website links in our text data, so we need to clean (remove) these \"noises\" before doing further analysis.\n",
        "\n",
        "Here are the data cleaning process step by step:\n",
        "\n",
        "- Import necessary libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0496f9ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk; \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from textblob import TextBlob\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc56a5c0",
      "metadata": {},
      "source": [
        "- Load the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bba1bf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets= pd.read_csv('../code/labeled_set.csv', encoding = \"ISO-8859-1\")\n",
        "tweets.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "962783de",
      "metadata": {},
      "source": [
        "- Remove empty space in columns if there is any. Rename the column name and check the data type of each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a9bf76",
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets.columns = tweets.columns.str.replace(' ', '')\n",
        "tweets.rename(columns={'text': 'tweets'}, inplace=True)\n",
        "tweets.dtypes\n",
        "tweets.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54768f2c",
      "metadata": {},
      "source": [
        "- Remove the \"noises\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c7eda6",
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets['tweets'] = tweets['tweets'].str.replace('[^\\w\\s]', '') # Remove all special symbols & characters\n",
        "tweets['tweets'] = tweets['tweets'].str.replace('_', '') # Remove all underscores\n",
        "tweets['tweets'] = tweets['tweets'].str.replace('http[^\\s]*',\"\") # Remove all words that start with \"http\"\n",
        "tweets['tweets'] = tweets['tweets'].astype(str).str.lower() # Make all words in lower case\n",
        "tweets.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86cb1601",
      "metadata": {},
      "source": [
        "- Save the clean dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8b12b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# tweets.to_csv('cleaned_tweets.csv')\n",
        "# tweets.to_html('cleaned_tweets.html', classes='table table-stripped')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7815ff",
      "metadata": {},
      "source": [
        "- Now we have the clean dataset so we can do some further analysis and some EDA.\n",
        "\n",
        "- Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "293053b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "regexp = RegexpTokenizer('\\w+')\n",
        "\n",
        "tweets['tweets_token']=tweets['tweets'].apply(regexp.tokenize)\n",
        "tweets.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8995a960",
      "metadata": {},
      "source": [
        "- Remove infrequent words. We first change the format of tweets_token to strings and keep only words which are no shorter than 2 letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4814b59",
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets['tweets_string'] = tweets['tweets_token'].apply(lambda x: ' '.join([item for item in x if len(item)>=2]))\n",
        "tweets.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca2f437",
      "metadata": {},
      "source": [
        "- Create a list of all words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f152f84c",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_words = ' '.join([word for word in tweets['tweets_string']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446696e7",
      "metadata": {},
      "source": [
        "- Tokenize all_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124e8498",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_words = nltk.tokenize.word_tokenize(all_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "107e8d7c",
      "metadata": {},
      "source": [
        "- Create a frequency distribution which records the number of times each word has occurred:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9bc6f8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(tokenized_words)\n",
        "fdist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc877bb",
      "metadata": {},
      "source": [
        "- Now we can use our fdist dictionary to drop words which occur less than a certain amount of times (usually we use a value of 3 or 4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121cec50",
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets['tweets_string_fdist'] = tweets['tweets_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] >= 4 ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "778a336d",
      "metadata": {},
      "source": [
        "- Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6deebdf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lem = WordNetLemmatizer()\n",
        "tweets['tweets_string_lem'] = tweets['tweets_string_fdist'].apply(wordnet_lem.lemmatize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138afc3e",
      "metadata": {},
      "source": [
        "- Check if the columns are equal so we might not need to do the lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a706659f",
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets['is_equal']= (tweets['tweets_string_fdist']==tweets['tweets_string_lem'])\n",
        "tweets.is_equal.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbef1f9b",
      "metadata": {},
      "source": [
        "- Let's create a word cloud to see what are the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c0c332c",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_words_lem = ' '.join([word for word in tweets['tweets_string_lem']])\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=600, \n",
        "                     height=400, \n",
        "                     random_state=2, \n",
        "                     max_font_size=100).generate(all_words_lem)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd95fab",
      "metadata": {},
      "source": [
        "- Frequency distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5073acc",
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "words = nltk.word_tokenize(all_words_lem)\n",
        "fd = FreqDist(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3448cdfc",
      "metadata": {},
      "source": [
        "- We can list the top 15 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fb7405",
      "metadata": {},
      "outputs": [],
      "source": [
        "fd.most_common(15)\n",
        "fd.tabulate(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18758bd",
      "metadata": {},
      "source": [
        "- Now we can make a plot of the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d535c81",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_30 = fd.most_common(30)\n",
        "\n",
        "# Create pandas series to make plotting easier\n",
        "fdist = pd.Series(dict(top_30))\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "tweets_top30_word_barplot = sns.barplot(y=fdist.index, x=fdist.values, color='pink')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5848edd41bd0e1d918c82d233b3526a43c6228f12035757ca34703663b40f88a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
